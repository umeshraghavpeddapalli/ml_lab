{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWZdVYskZZuB",
        "outputId": "86ed51bb-73d1-4fd0-9a4f-bed5071d3c2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Actual: It Stem: it\n",
            "Actual: originated Stem: origin\n",
            "Actual: from Stem: from\n",
            "Actual: the Stem: the\n",
            "Actual: idea Stem: idea\n",
            "Actual: that Stem: that\n",
            "Actual: there Stem: there\n",
            "Actual: are Stem: are\n",
            "Actual: readers Stem: reader\n",
            "Actual: who Stem: who\n",
            "Actual: prefer Stem: prefer\n",
            "Actual: learning Stem: learn\n",
            "Actual: new Stem: new\n",
            "Actual: skills Stem: skill\n",
            "Actual: from Stem: from\n",
            "Actual: the Stem: the\n",
            "Actual: comforts Stem: comfort\n",
            "Actual: of Stem: of\n",
            "Actual: their Stem: their\n",
            "Actual: drawing Stem: draw\n",
            "Actual: rooms Stem: room\n",
            "Actual: It Stem: it\n",
            "Actual: originated Stem: origin\n",
            "Actual: from Stem: from\n",
            "Actual: the Stem: the\n",
            "Actual: idea Stem: idea\n",
            "Actual: that Stem: that\n",
            "Actual: there Stem: there\n",
            "Actual: are Stem: are\n",
            "Actual: readers Stem: reader\n",
            "Actual: who Stem: who\n",
            "Actual: prefer Stem: prefer\n",
            "Actual: learning Stem: learn\n",
            "Actual: new Stem: new\n",
            "Actual: skills Stem: skill\n",
            "Actual: from Stem: from\n",
            "Actual: the Stem: the\n",
            "Actual: comforts Stem: comfort\n",
            "Actual: of Stem: of\n",
            "Actual: their Stem: their\n",
            "Actual: drawing Stem: draw\n",
            "Actual: rooms Stem: room\n",
            "Porter stemmer \n",
            "cat\n",
            "troubl\n",
            "troubl\n",
            "troubl\n",
            "Lancaster Stemmer\n",
            "cat\n",
            "troubl\n",
            "troubl\n",
            "troubl\n",
            "[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.'), ('The', 'DT'), ('sun', 'NN'), ('is', 'VBZ'), ('shining', 'VBG'), ('brightly', 'RB'), ('in', 'IN'), ('the', 'DT'), ('clear', 'JJ'), ('blue', 'NN'), ('sky', 'NN'), ('.', '.')]\n",
            "['The', 'quick', 'brown', 'fox', 'jump', 'over', 'the', 'lazy', 'dog', '.', 'The', 'sun', 'be', 'shin', 'brightly', 'in', 'the', 'clear', 'blue', 'sky', '.']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "porter_stemmer = PorterStemmer()\n",
        "\n",
        "word_data = 'It originated from the idea that there are readers who prefer learning new skills from the comforts of their drawing rooms'\n",
        "##word_data=\"Your suit was better than mine“\n",
        "# First Word tokenization\n",
        "nltk_tokens= nltk.word_tokenize(word_data)\n",
        "#Next find the roots of the word\n",
        "for w in nltk_tokens:\n",
        "   print (\"Actual: %s Stem: %s\"  % (w,porter_stemmer.stem(w)))\n",
        "\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "porter_stemmer = PorterStemmer()\n",
        "\n",
        "word_data = \"It originated from the idea that there are readers who prefer learning new skills from the comforts of their drawing rooms\"\n",
        "##word_data=\"Your suit was better than mine“\n",
        "# First Word tokenization\n",
        "nltk_tokens = nltk.word_tokenize(word_data)\n",
        "#Next find the roots of the word\n",
        "for w in nltk_tokens:\n",
        "       print (\"Actual: %s Stem: %s\"  % (w,porter_stemmer.stem(w)))\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import LancasterStemmer\n",
        "\n",
        "porter = PorterStemmer()\n",
        "lancaster=LancasterStemmer()\n",
        "#proide a word to be stemmed\n",
        "print(\"Porter stemmer \")\n",
        "print(porter.stem('cats'))\n",
        "print(porter.stem('trouble'))\n",
        "print(porter.stem('troubling'))\n",
        "print(porter.stem('troubled'))\n",
        "print('Lancaster Stemmer')\n",
        "print(lancaster.stem('cats'))\n",
        "print(lancaster.stem('trouble'))\n",
        "print(lancaster.stem('troubling'))\n",
        "print(lancaster.stem('troubled'))\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "w = WordNetLemmatizer()\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "def findpos(x):\n",
        "    t = 'n'\n",
        "    if x.startswith(\"V\"):\n",
        "        t = \"v\"\n",
        "    elif x.startswith(\"J\"):\n",
        "        t = \"a\"\n",
        "    elif x.startswith(\"R\"):\n",
        "        t = \"r\"\n",
        "    elif x.startswith(\"N\"):\n",
        "        t = \"n\"\n",
        "    return t\n",
        "\n",
        "word_data = \"The quick brown fox jumps over the lazy dog. The sun is shining brightly in the clear blue sky.\"\n",
        "##word_data=\"Your suit was better than mine\"# First Word tokenization\n",
        "tokens = nltk.word_tokenize(word_data)\n",
        "pos=nltk.pos_tag(tokens)\n",
        "print(pos)\n",
        "nltk.download('wordnet')\n",
        "lemma = []\n",
        "for i in range(len(pos)):\n",
        "    x = findpos(pos[i][1])\n",
        "    lemma.append(w.lemmatize(pos[i][0], x))\n",
        "print(lemma)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
